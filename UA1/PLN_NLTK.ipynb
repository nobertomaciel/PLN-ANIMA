{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobertomaciel/PLN-ANIMA/blob/main/UA1/PLN_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAYvAD1fbyM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ea4214-ff6e-45ec-eeda-da9e77fb46e3"
      },
      "source": [
        "# instala a o Natural Language Tool Kit (https://www.nltk.org/)\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2o5GeyRb2DL"
      },
      "source": [
        "# Importações\n",
        "# 1. Importa o NLTK\n",
        "# 2. Importa o corpus “Machado de Assis” disponível no NLTK, um dos conjuntos de textos literários em português incluídos na biblioteca.\n",
        "# 3. Importa as funções sent_tokenize() e word_tokenize() do módulo nltk.tokenize\n",
        "# 4. Importa o CountVectorizer da biblioteca skleanr, que faz a conversão de texto em números, criando uma matriz de contagem de palavras (bag of words).\n",
        "# 5. Importa a biblioteca que permite acessar arquivos na web\n",
        "import nltk\n",
        "from nltk.corpus import machado\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz o download dos pacotes do NLTK\n",
        "# 1. Lista de palavras de parada (stopwords) — palavras muito comuns que geralmente são removidas de textos porque têm pouco valor semântico\n",
        "# 2. Tokenizador Punkt, um modelo estatístico que divide textos em sentenças e palavras.\n",
        "# 3. Tabelas de idioma que ele usa para tokenizar corretamente.\n",
        "# 4. Corpus de Machado de Assis (um conjunto de textos literários em português incluído no NLTK).\n",
        "# A função sent_tokenize() serve para dividir o texto em sentenças (frases) — ou seja, faz a tokenização em nível de sentença.\n",
        "# A função word_tokenize() é usada para dividir um texto em palavras (tokens), ou seja, realizar a tokenização lexical.\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('machado')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umBmdOPj2_ew",
        "outputId": "acdc9381-ab6b-48fa-8f67-ebda89496608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baDTZe33cVcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d0cccc-f6cd-45f4-ce74-950d0175ee12"
      },
      "source": [
        "# Faz o pré-processamento do texto:\n",
        "# [ABANDONADO: apresentando erro no pacote nltk.corpus machado] 1. Abre o arquivo 'romance/marm05.txt'atribui à variável f\n",
        "# [ABANDONADO: erro] 2. Faz a leitura do arquivo e atribui o conteúdo à variável texto\n",
        "# 1. [ABANDONADO: usando string] 3. Seleciona apenas uma parte do texto (um recorte específico do arquivo 'romance/marm05.txt') e substitui na variável texto\n",
        "# 2. [4.] Remove as quebras de linha (\\n), substituindo-as por espaços.\n",
        "# 3. [5.] Imprime o texto pré-processado\n",
        "# with machado.open('romance/marm05.txt') as f:\n",
        "#     texto = f.read()\n",
        "texto = \"Como este apelido de Cubas lhe cheirasse excessivamente a tanoaria, alegava meu pai, bisneto de Damião, que o dito apelido fora dado a um cavaleiro, herói nas jornadas da África, em prêmio da façanha que praticou, arrebatando trezentas cubas aos mouros. Meu pai era homem de imaginação; escapou à tanoaria nas asas de um calembour. Era um bom caráter, meu pai, varão digno e leal como poucos. Tinha, é verdade, uns fumos de pacholice; mas quem não é um pouco pachola nesse mundo? Releva notar que ele não recorreu à inventiva senão depois de experimentar a falsificação; primeiramente, entroncou-se na família daquele meu famoso homônimo, o capitão-mor, Brás Cubas, que fundou a vila de São Vicente, onde morreu em 1592, e por esse motivo é que me deu o nome de Brás. Opôs-se-lhe, porém, a família do capitão-mor, e foi então que ele imaginou as trezentas cubas mouriscas.\"\n",
        "texto = texto.replace('\\n',' ')\n",
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como este apelido de Cubas lhe cheirasse excessivamente a tanoaria, alegava meu pai, bisneto de Damião, que o dito apelido fora dado a um cavaleiro, herói nas jornadas da África, em prêmio da façanha que praticou, arrebatando trezentas cubas aos mouros. Meu pai era homem de imaginação; escapou à tanoaria nas asas de um calembour. Era um bom caráter, meu pai, varão digno e leal como poucos. Tinha, é verdade, uns fumos de pacholice; mas quem não é um pouco pachola nesse mundo? Releva notar que ele não recorreu à inventiva senão depois de experimentar a falsificação; primeiramente, entroncou-se na família daquele meu famoso homônimo, o capitão-mor, Brás Cubas, que fundou a vila de São Vicente, onde morreu em 1592, e por esse motivo é que me deu o nome de Brás. Opôs-se-lhe, porém, a família do capitão-mor, e foi então que ele imaginou as trezentas cubas mouriscas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWGeg3tMcrT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bfde28-3af4-4294-e448-16af8474cf1c"
      },
      "source": [
        "# Esse trecho de código faz a tokenização de sentenças (ou seja, divide um texto em frases) e depois exibe cada sentença numerada.\n",
        "# 1. Atribui o texto tokenizado à variável sentencas como uma lista\n",
        "# 2. Faz uma iteração em sentencas, extraíndo e numerando cada uma das frases\n",
        "# 3. Imprime as sentenças na tela, devidamente enumeradas\n",
        "sentencas = sent_tokenize(texto, language='portuguese')\n",
        "for i,sent in enumerate(sentencas):\n",
        "     print(i,'-',sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 - Como este apelido de Cubas lhe cheirasse excessivamente a tanoaria, alegava meu pai, bisneto de Damião, que o dito apelido fora dado a um cavaleiro, herói nas jornadas da África, em prêmio da façanha que praticou, arrebatando trezentas cubas aos mouros.\n",
            "1 - Meu pai era homem de imaginação; escapou à tanoaria nas asas de um calembour.\n",
            "2 - Era um bom caráter, meu pai, varão digno e leal como poucos.\n",
            "3 - Tinha, é verdade, uns fumos de pacholice; mas quem não é um pouco pachola nesse mundo?\n",
            "4 - Releva notar que ele não recorreu à inventiva senão depois de experimentar a falsificação; primeiramente, entroncou-se na família daquele meu famoso homônimo, o capitão-mor, Brás Cubas, que fundou a vila de São Vicente, onde morreu em 1592, e por esse motivo é que me deu o nome de Brás.\n",
            "5 - Opôs-se-lhe, porém, a família do capitão-mor, e foi então que ele imaginou as trezentas cubas mouriscas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUyGxsqkjRKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c2b178-2744-41b7-e4da-67097b5197b6"
      },
      "source": [
        "# Continua com a tokenização, só que agora lexical (dividindo em palavras)\n",
        "# 1. Atribui as palavras tokenizadas à variável palavras como uma lista\n",
        "# 2. Imprime o conteúdo de palavras na tela\n",
        "palavras = word_tokenize(texto, language='portuguese')\n",
        "print(palavras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Como', 'este', 'apelido', 'de', 'Cubas', 'lhe', 'cheirasse', 'excessivamente', 'a', 'tanoaria', ',', 'alegava', 'meu', 'pai', ',', 'bisneto', 'de', 'Damião', ',', 'que', 'o', 'dito', 'apelido', 'fora', 'dado', 'a', 'um', 'cavaleiro', ',', 'herói', 'nas', 'jornadas', 'da', 'África', ',', 'em', 'prêmio', 'da', 'façanha', 'que', 'praticou', ',', 'arrebatando', 'trezentas', 'cubas', 'aos', 'mouros', '.', 'Meu', 'pai', 'era', 'homem', 'de', 'imaginação', ';', 'escapou', 'à', 'tanoaria', 'nas', 'asas', 'de', 'um', 'calembour', '.', 'Era', 'um', 'bom', 'caráter', ',', 'meu', 'pai', ',', 'varão', 'digno', 'e', 'leal', 'como', 'poucos', '.', 'Tinha', ',', 'é', 'verdade', ',', 'uns', 'fumos', 'de', 'pacholice', ';', 'mas', 'quem', 'não', 'é', 'um', 'pouco', 'pachola', 'nesse', 'mundo', '?', 'Releva', 'notar', 'que', 'ele', 'não', 'recorreu', 'à', 'inventiva', 'senão', 'depois', 'de', 'experimentar', 'a', 'falsificação', ';', 'primeiramente', ',', 'entroncou-se', 'na', 'família', 'daquele', 'meu', 'famoso', 'homônimo', ',', 'o', 'capitão-mor', ',', 'Brás', 'Cubas', ',', 'que', 'fundou', 'a', 'vila', 'de', 'São', 'Vicente', ',', 'onde', 'morreu', 'em', '1592', ',', 'e', 'por', 'esse', 'motivo', 'é', 'que', 'me', 'deu', 'o', 'nome', 'de', 'Brás', '.', 'Opôs-se-lhe', ',', 'porém', ',', 'a', 'família', 'do', 'capitão-mor', ',', 'e', 'foi', 'então', 'que', 'ele', 'imaginou', 'as', 'trezentas', 'cubas', 'mouriscas', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRb_tHPlVjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52381fb-a37f-49f8-fce7-0b7d08bbd717"
      },
      "source": [
        "# Refaz a tokenização em nível de sentença, só que, dessa vez, com as palavras divididas\n",
        "# 1. Atribui o texto tokenizado à variável sentencas como uma lista\n",
        "# 2. Faz uma iteração em sentencas, extraíndo e numerando cada uma das frases\n",
        "# 3. Imprime as palavras tokenizadas como uma lista\n",
        "sentencas = sent_tokenize(texto, language='portuguese')\n",
        "for i,sent in enumerate(sentencas):\n",
        "     print(i,'-',word_tokenize(sent, language='portuguese'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 - ['Como', 'este', 'apelido', 'de', 'Cubas', 'lhe', 'cheirasse', 'excessivamente', 'a', 'tanoaria', ',', 'alegava', 'meu', 'pai', ',', 'bisneto', 'de', 'Damião', ',', 'que', 'o', 'dito', 'apelido', 'fora', 'dado', 'a', 'um', 'cavaleiro', ',', 'herói', 'nas', 'jornadas', 'da', 'África', ',', 'em', 'prêmio', 'da', 'façanha', 'que', 'praticou', ',', 'arrebatando', 'trezentas', 'cubas', 'aos', 'mouros', '.']\n",
            "1 - ['Meu', 'pai', 'era', 'homem', 'de', 'imaginação', ';', 'escapou', 'à', 'tanoaria', 'nas', 'asas', 'de', 'um', 'calembour', '.']\n",
            "2 - ['Era', 'um', 'bom', 'caráter', ',', 'meu', 'pai', ',', 'varão', 'digno', 'e', 'leal', 'como', 'poucos', '.']\n",
            "3 - ['Tinha', ',', 'é', 'verdade', ',', 'uns', 'fumos', 'de', 'pacholice', ';', 'mas', 'quem', 'não', 'é', 'um', 'pouco', 'pachola', 'nesse', 'mundo', '?']\n",
            "4 - ['Releva', 'notar', 'que', 'ele', 'não', 'recorreu', 'à', 'inventiva', 'senão', 'depois', 'de', 'experimentar', 'a', 'falsificação', ';', 'primeiramente', ',', 'entroncou-se', 'na', 'família', 'daquele', 'meu', 'famoso', 'homônimo', ',', 'o', 'capitão-mor', ',', 'Brás', 'Cubas', ',', 'que', 'fundou', 'a', 'vila', 'de', 'São', 'Vicente', ',', 'onde', 'morreu', 'em', '1592', ',', 'e', 'por', 'esse', 'motivo', 'é', 'que', 'me', 'deu', 'o', 'nome', 'de', 'Brás', '.']\n",
            "5 - ['Opôs-se-lhe', ',', 'porém', ',', 'a', 'família', 'do', 'capitão-mor', ',', 'e', 'foi', 'então', 'que', 'ele', 'imaginou', 'as', 'trezentas', 'cubas', 'mouriscas', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQPscxhxoKDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbb75db-9b33-479c-c3d1-43a611280bfb"
      },
      "source": [
        "# Cria uma lista de caracteres especiais\n",
        "# 1. Importa a biblioteca string (padrão do Python), que contém funções e constantes úteis para manipulação de textos e caracteres\n",
        "# 2. Extrai apenas os caracteres especiais, atribuindo à variáel punctuations como uma lista\n",
        "# 3. Imprime o conteúdo de punctuations\n",
        "import string\n",
        "punctuations = list(string.punctuation)\n",
        "print(punctuations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIcRWVjIoc7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53570d3f-cdf1-4b92-9277-568d336b8516"
      },
      "source": [
        "# Faz o mesmo processo anterior, só que retirando os caracteres especiais\n",
        "sentencas = sent_tokenize(texto, language='portuguese')\n",
        "for i,sent in enumerate(sentencas):\n",
        "    sent_sem_pontuacao = [i for i in word_tokenize(sent, language='portuguese') if i not in punctuations]\n",
        "    print(i,'-',sent_sem_pontuacao)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 - ['Como', 'este', 'apelido', 'de', 'Cubas', 'lhe', 'cheirasse', 'excessivamente', 'a', 'tanoaria', 'alegava', 'meu', 'pai', 'bisneto', 'de', 'Damião', 'que', 'o', 'dito', 'apelido', 'fora', 'dado', 'a', 'um', 'cavaleiro', 'herói', 'nas', 'jornadas', 'da', 'África', 'em', 'prêmio', 'da', 'façanha', 'que', 'praticou', 'arrebatando', 'trezentas', 'cubas', 'aos', 'mouros']\n",
            "1 - ['Meu', 'pai', 'era', 'homem', 'de', 'imaginação', 'escapou', 'à', 'tanoaria', 'nas', 'asas', 'de', 'um', 'calembour']\n",
            "2 - ['Era', 'um', 'bom', 'caráter', 'meu', 'pai', 'varão', 'digno', 'e', 'leal', 'como', 'poucos']\n",
            "3 - ['Tinha', 'é', 'verdade', 'uns', 'fumos', 'de', 'pacholice', 'mas', 'quem', 'não', 'é', 'um', 'pouco', 'pachola', 'nesse', 'mundo']\n",
            "4 - ['Releva', 'notar', 'que', 'ele', 'não', 'recorreu', 'à', 'inventiva', 'senão', 'depois', 'de', 'experimentar', 'a', 'falsificação', 'primeiramente', 'entroncou-se', 'na', 'família', 'daquele', 'meu', 'famoso', 'homônimo', 'o', 'capitão-mor', 'Brás', 'Cubas', 'que', 'fundou', 'a', 'vila', 'de', 'São', 'Vicente', 'onde', 'morreu', 'em', '1592', 'e', 'por', 'esse', 'motivo', 'é', 'que', 'me', 'deu', 'o', 'nome', 'de', 'Brás']\n",
            "5 - ['Opôs-se-lhe', 'porém', 'a', 'família', 'do', 'capitão-mor', 'e', 'foi', 'então', 'que', 'ele', 'imaginou', 'as', 'trezentas', 'cubas', 'mouriscas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwx8JtLlltom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ffa649-69b1-463e-d815-5a1101b5a923"
      },
      "source": [
        "# Seleciona as palavras que serão utilizadas como stopwords (que devem ser ignoradas no processamento)\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'não', 'nas', 'nem', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'seu', 'seus', 'só', 'somos', 'sou', 'sua', 'suas', 'também', 'te', 'tem', 'tém', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terá', 'terão', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tu', 'tua', 'tuas', 'um', 'uma', 'você', 'vocês', 'vos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvWt43ljnDUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c56056-64dc-4240-80cd-44af269b254d"
      },
      "source": [
        "# Faz o mesmo processo de tokenização, só que dessa vez aplicando tanto a remoção de caracteres quanto as stopwords\n",
        "sentencas = sent_tokenize(texto, language='portuguese')\n",
        "for i,sent in enumerate(sentencas):\n",
        "    sent_sem_pontuacao = [i for i in word_tokenize(sent, language='portuguese') if i not in punctuations]\n",
        "    sent_sem_stopwords = [i for i in sent_sem_pontuacao if i not in stopwords]\n",
        "    print(i,'-',sent_sem_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 - ['Como', 'apelido', 'Cubas', 'cheirasse', 'excessivamente', 'tanoaria', 'alegava', 'pai', 'bisneto', 'Damião', 'dito', 'apelido', 'dado', 'cavaleiro', 'herói', 'jornadas', 'África', 'prêmio', 'façanha', 'praticou', 'arrebatando', 'trezentas', 'cubas', 'mouros']\n",
            "1 - ['Meu', 'pai', 'homem', 'imaginação', 'escapou', 'tanoaria', 'asas', 'calembour']\n",
            "2 - ['Era', 'bom', 'caráter', 'pai', 'varão', 'digno', 'leal', 'poucos']\n",
            "3 - ['Tinha', 'verdade', 'uns', 'fumos', 'pacholice', 'pouco', 'pachola', 'nesse', 'mundo']\n",
            "4 - ['Releva', 'notar', 'recorreu', 'inventiva', 'senão', 'experimentar', 'falsificação', 'primeiramente', 'entroncou-se', 'família', 'daquele', 'famoso', 'homônimo', 'capitão-mor', 'Brás', 'Cubas', 'fundou', 'vila', 'São', 'Vicente', 'onde', 'morreu', '1592', 'motivo', 'deu', 'nome', 'Brás']\n",
            "5 - ['Opôs-se-lhe', 'porém', 'família', 'capitão-mor', 'então', 'imaginou', 'trezentas', 'cubas', 'mouriscas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzeZRSCSC6Uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4500c142-e771-45ba-d7a8-8ac6e4671082"
      },
      "source": [
        "# 1. Cria o vetorizador Bag of Words (BoW), que conta as ocorrências de cada palavra e define o tipo de n-gramas (aqui, unigramas)\n",
        "# 2. Realiza o aprendizado do vocabulário (fit) e transforma as sentenças em vetores numéricos (transform)\n",
        "# 3. Imprime o formato da matriz BoW: linhas = sentenças, colunas = palavras únicas\n",
        "# 4. Imprime o vocabulário aprendido e o número total de palavras distintas\n",
        "# 5. Imprime a primeira sentença transformada em vetor numérico (array)\n",
        "# 6. Imprime a segunda sentença transformada em vetor numérico (array)\n",
        "cv = CountVectorizer(min_df=0.0,max_df=1.0,binary=False,ngram_range=(1,1))\n",
        "bow = cv.fit_transform(sentencas)\n",
        "print('BoW Shape:',bow.shape)\n",
        "print('BoW Vocabulário (',len(cv.vocabulary_),'palavras ):',cv.vocabulary_)\n",
        "print('Primeira sentença:',bow[0].toarray())\n",
        "print('Segunda sentença:',bow[1].toarray())\n",
        "\n",
        "# Neste ponto nós temos a variável bow, cuja principal utilidade é alimentar modelos de aprendizado de máquina para tarefas como:\n",
        "# a) Classificação de texto (ex: spam vs. não-spam)\n",
        "# b) Análise de sentimento (positivo, negativo)\n",
        "# c) Agrupamento de documentos (clustering)\n",
        "# d) Detecção de temas (topic modeling)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Shape: (6, 98)\n",
            "BoW Vocabulário ( 98 palavras ): {'como': 15, 'este': 34, 'apelido': 3, 'de': 21, 'cubas': 16, 'lhe': 53, 'cheirasse': 14, 'excessivamente': 35, 'tanoaria': 88, 'alegava': 1, 'meu': 56, 'pai': 73, 'bisneto': 7, 'damião': 19, 'que': 81, 'dito': 25, 'fora': 42, 'dado': 18, 'um': 91, 'cavaleiro': 13, 'herói': 45, 'nas': 64, 'jornadas': 51, 'da': 17, 'áfrica': 97, 'em': 28, 'prêmio': 80, 'façanha': 40, 'praticou': 78, 'arrebatando': 4, 'trezentas': 90, 'aos': 2, 'mouros': 61, 'era': 31, 'homem': 46, 'imaginação': 48, 'escapou': 32, 'asas': 6, 'calembour': 10, 'bom': 8, 'caráter': 12, 'varão': 93, 'digno': 24, 'leal': 52, 'poucos': 77, 'tinha': 89, 'verdade': 94, 'uns': 92, 'fumos': 43, 'pacholice': 72, 'mas': 54, 'quem': 82, 'não': 68, 'pouco': 76, 'pachola': 71, 'nesse': 65, 'mundo': 62, 'releva': 84, 'notar': 67, 'ele': 27, 'recorreu': 83, 'inventiva': 50, 'senão': 86, 'depois': 22, 'experimentar': 36, 'falsificação': 37, 'primeiramente': 79, 'entroncou': 29, 'se': 85, 'na': 63, 'família': 39, 'daquele': 20, 'famoso': 38, 'homônimo': 47, 'capitão': 11, 'mor': 57, 'brás': 9, 'fundou': 44, 'vila': 96, 'são': 87, 'vicente': 95, 'onde': 69, 'morreu': 58, '1592': 0, 'por': 74, 'esse': 33, 'motivo': 59, 'me': 55, 'deu': 23, 'nome': 66, 'opôs': 70, 'porém': 75, 'do': 26, 'foi': 41, 'então': 30, 'imaginou': 49, 'as': 5, 'mouriscas': 60}\n",
            "Primeira sentença: [[0 1 1 2 1 0 0 1 0 0 0 0 0 1 1 1 2 2 1 1 0 2 0 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            "  0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 1 0 1 2 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1]]\n",
            "Segunda sentença: [[0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev1DvLGPGwWU"
      },
      "source": [
        "FIM"
      ]
    }
  ]
}