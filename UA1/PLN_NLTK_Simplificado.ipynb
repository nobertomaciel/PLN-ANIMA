{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobertomaciel/PLN-ANIMA/blob/main/UA1/PLN_NLTK_Simplificado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38dbb0df",
      "metadata": {
        "id": "38dbb0df"
      },
      "source": [
        "# PLN com NLTK — Versão Simplificada\n",
        "Este notebook realiza tokenização, limpeza, remoção de stopwords e vetorização (Bag of Words) em português."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a573919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a573919",
        "outputId": "decb4322-8945-4101-9ed3-f0db27201e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Instala o Natural Language Toolkit (NLTK)\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a06e60a",
      "metadata": {
        "id": "8a06e60a"
      },
      "outputs": [],
      "source": [
        "# Importações principais\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a034941",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a034941",
        "outputId": "8f093d20-d85b-4061-d7f3-883c12da0035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Faz o download dos recursos necessários do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257c0aaa",
      "metadata": {
        "id": "257c0aaa"
      },
      "outputs": [],
      "source": [
        "# Texto de exemplo\n",
        "texto = \"\"\"Como este apelido de Cubas lhe cheirasse excessivamente a tanoaria, alegava meu pai, bisneto de Damião,\n",
        "que o dito apelido fora dado a um cavaleiro, herói nas jornadas da África, em prêmio da façanha que praticou,\n",
        "arrebatando trezentas cubas aos mouros. Meu pai era homem de imaginação; escapou à tanoaria nas asas de um calembour.\n",
        "Era um bom caráter, meu pai, varão digno e leal como poucos. Tinha, é verdade, uns fumos de pacholice; mas quem não é um pouco pachola nesse mundo?\"\"\"\n",
        "\n",
        "# Limita o texto para fins de demonstração\n",
        "texto = texto[:950].replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd70445b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd70445b",
        "outputId": "bab4a10e-829d-403d-d47e-a3ee3e1f0a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de sentenças: 4\n",
            "Total de tokens: 99\n",
            "Exemplo de tokens: ['Como', 'este', 'apelido', 'de', 'Cubas', 'lhe', 'cheirasse', 'excessivamente', 'a', 'tanoaria', ',', 'alegava', 'meu', 'pai', ',', 'bisneto', 'de', 'Damião', ',', 'que']\n"
          ]
        }
      ],
      "source": [
        "# Tokeniza o texto em sentenças e depois em palavras\n",
        "sentencas = sent_tokenize(texto, language='portuguese')\n",
        "tokens = word_tokenize(texto, language='portuguese')\n",
        "\n",
        "print(\"Total de sentenças:\", len(sentencas))\n",
        "print(\"Total de tokens:\", len(tokens))\n",
        "print(\"Exemplo de tokens:\", tokens[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ffc4ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29ffc4ec",
        "outputId": "66196648-1300-40f7-a13d-2fc610bf3fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens após remoção de pontuação: ['Como', 'este', 'apelido', 'de', 'Cubas', 'lhe', 'cheirasse', 'excessivamente', 'a', 'tanoaria', 'alegava', 'meu', 'pai', 'bisneto', 'de', 'Damião', 'que', 'o', 'dito', 'apelido']\n"
          ]
        }
      ],
      "source": [
        "# Remove pontuação e caracteres especiais\n",
        "tokens_limpos = [t for t in tokens if t not in string.punctuation]\n",
        "\n",
        "print(\"Tokens após remoção de pontuação:\", tokens_limpos[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25ef309",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25ef309",
        "outputId": "d7597f8a-ac0a-4f54-bf07-d4b58bedd486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens após remoção de stopwords: ['apelido', 'Cubas', 'cheirasse', 'excessivamente', 'tanoaria', 'alegava', 'pai', 'bisneto', 'Damião', 'dito', 'apelido', 'dado', 'cavaleiro', 'herói', 'jornadas', 'África', 'prêmio', 'façanha', 'praticou', 'arrebatando']\n"
          ]
        }
      ],
      "source": [
        "# Remove palavras irrelevantes (stopwords)\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "tokens_sem_stop = [t for t in tokens_limpos if t.lower() not in stop_words]\n",
        "\n",
        "print(\"Tokens após remoção de stopwords:\", tokens_sem_stop[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9522ac03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9522ac03",
        "outputId": "aae83501-31d1-41fc-abe0-a7ad16b9f8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Shape: (4, 57)\n",
            "Vocabulário ( 57 palavras): {'como': 11, 'este': 22, 'apelido': 2, 'de': 16, 'cubas': 12, 'lhe': 32, 'cheirasse': 10, 'excessivamente': 23, 'tanoaria': 49, 'alegava': 0, 'meu': 34, 'pai': 42, 'bisneto': 5, 'damião': 15, 'que': 47, 'dito': 18, 'fora': 25, 'dado': 14, 'um': 52, 'cavaleiro': 9, 'herói': 27, 'nas': 37, 'jornadas': 30, 'da': 13, 'áfrica': 56, 'em': 19, 'prêmio': 46, 'façanha': 24, 'praticou': 45, 'arrebatando': 3, 'trezentas': 51, 'aos': 1, 'mouros': 35, 'era': 20, 'homem': 28, 'imaginação': 29, 'escapou': 21, 'asas': 4, 'calembour': 7, 'bom': 6, 'caráter': 8, 'varão': 54, 'digno': 17, 'leal': 31, 'poucos': 44, 'tinha': 50, 'verdade': 55, 'uns': 53, 'fumos': 26, 'pacholice': 41, 'mas': 33, 'quem': 48, 'não': 39, 'pouco': 43, 'pachola': 40, 'nesse': 38, 'mundo': 36}\n",
            "\n",
            " Sentença 0 :\n",
            " 1 1 2 1 0 1 0 0 0 1 1 1 2 2 1 1 2 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 2 0 1 0 1 1 0 0 0 1\n",
            "\n",
            " Sentença 1 :\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            "\n",
            " Sentença 2 :\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            "\n",
            " Sentença 3 :\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0\n"
          ]
        }
      ],
      "source": [
        "# 1. Cria o vetorizador Bag of Words (BoW)\n",
        "cv = CountVectorizer(min_df=0.0, max_df=1.0, binary=False, ngram_range=(1,1))\n",
        "\n",
        "# 2. Gera o vocabulário e transforma as sentenças (esta variável bow é a que será utilizada em algum algoritmo de machine learning)\n",
        "bow = cv.fit_transform(sentencas)\n",
        "\n",
        "# 3. Exibe resultados\n",
        "print('BoW Shape:', bow.shape)\n",
        "print('Vocabulário (', len(cv.vocabulary_), 'palavras):', cv.vocabulary_)\n",
        "for n,sent in enumerate(bow):\n",
        "    print('\\n','Sentença',n,':\\n', \" \".join(map(str, sent.toarray().flatten())))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A variável **bow** tem a principal utilidade de alimentar modelos de aprendizado de máquina para tarefas como:\n",
        "*   Classificação de texto (ex: spam vs. não-spam)\n",
        "*   Análise de sentimento (positivo, negativo)\n",
        "*   Agrupamento de documentos (clustering)\n",
        "*   Detecção de temas (topic modeling)"
      ],
      "metadata": {
        "id": "dQF5mEtQOsVa"
      },
      "id": "dQF5mEtQOsVa"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}