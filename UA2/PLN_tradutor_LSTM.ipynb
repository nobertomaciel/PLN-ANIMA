{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCaDFzxDuzImpw3q/+gC1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobertomaciel/PLN-ANIMA/blob/main/UA2/PLN_tradutor_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "3VFViUkCxrL9",
        "outputId": "11d419f0-e067-4fce-b483-d132df9ccfea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │      \u001b[38;5;34m1,344\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │      \u001b[38;5;34m1,664\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m98,816\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m),  │     \u001b[38;5;34m98,816\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m26\u001b[0m)     │      \u001b[38;5;34m3,354\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m203,994\u001b[0m (796.85 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">203,994</span> (796.85 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m203,994\u001b[0m (796.85 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">203,994</span> (796.85 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinamento concluído!\n",
            "\n",
            "===== TESTES DE TRADUÇÃO =====\n",
            "EN: hi\n",
            "PT: olá\n",
            "\n",
            "EN: i love you\n",
            "PT: eu te amo\n",
            "\n",
            "EN: good night\n",
            "PT: boa noite\n",
            "\n",
            "EN: how are you\n",
            "PT: como você está\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# TRADUTOR SIMPLES COM LSTM (SEQ2SEQ) – GOOGLE COLAB\n",
        "# =============================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1) Mini-dataset de exemplo (EN → PT)\n",
        "# -------------------------------------------------------------\n",
        "eng_sentences = [\n",
        "    \"hi\", \"how are you\", \"thanks\", \"i love you\", \"good morning\", \"good night\",\n",
        "    \"see you soon\", \"what is your name\", \"i am happy\", \"let's go\"\n",
        "]\n",
        "\n",
        "pt_sentences = [\n",
        "    \"olá\", \"como você está\", \"obrigado\", \"eu te amo\", \"bom dia\", \"boa noite\",\n",
        "    \"até logo\", \"qual é o seu nome\", \"estou feliz\", \"vamos lá\"\n",
        "]\n",
        "\n",
        "# Adicionar tokens especiais\n",
        "pt_sentences_input  = [\"<start> \" + s for s in pt_sentences]\n",
        "pt_sentences_target = [s + \" <end>\" for s in pt_sentences]\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Tokenização\n",
        "# -------------------------------------------------------------\n",
        "tokenizer_eng = Tokenizer()\n",
        "tokenizer_pt = Tokenizer()\n",
        "\n",
        "tokenizer_eng.fit_on_texts(eng_sentences)\n",
        "tokenizer_pt.fit_on_texts(pt_sentences_input + pt_sentences_target)\n",
        "\n",
        "seq_eng = tokenizer_eng.texts_to_sequences(eng_sentences)\n",
        "seq_pt_input  = tokenizer_pt.texts_to_sequences(pt_sentences_input)\n",
        "seq_pt_target = tokenizer_pt.texts_to_sequences(pt_sentences_target)\n",
        "\n",
        "max_eng = max(len(x) for x in seq_eng)\n",
        "max_pt  = max(len(x) for x in seq_pt_input)\n",
        "\n",
        "seq_eng = pad_sequences(seq_eng, maxlen=max_eng, padding='post')\n",
        "seq_pt_input = pad_sequences(seq_pt_input, maxlen=max_pt, padding='post')\n",
        "seq_pt_target = pad_sequences(seq_pt_target, maxlen=max_pt, padding='post')\n",
        "\n",
        "vocab_eng = len(tokenizer_eng.word_index) + 1\n",
        "vocab_pt  = len(tokenizer_pt.word_index) + 1\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3) Construção do modelo Seq2Seq com LSTM\n",
        "# -------------------------------------------------------------\n",
        "embedding_dim = 64\n",
        "latent_dim = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_eng,))\n",
        "enc_emb = Embedding(vocab_eng, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "_, h, c = encoder_lstm(enc_emb)\n",
        "\n",
        "encoder_states = [h, c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_pt,))\n",
        "dec_emb_layer = Embedding(vocab_pt, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(vocab_pt, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Modelo final (treinamento)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4) Treinamento\n",
        "# -------------------------------------------------------------\n",
        "model.fit([seq_eng, seq_pt_input], np.expand_dims(seq_pt_target, -1),\n",
        "          batch_size=2, epochs=300, verbose=0)\n",
        "\n",
        "print(\"\\nTreinamento concluído!\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5) Modelo para inferência (tradução)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# Encoder para inferência\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder para inferência\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, h2, c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2, h2, c2]\n",
        ")\n",
        "\n",
        "index_to_word_pt = {i: w for w, i in tokenizer_pt.word_index.items()}\n",
        "\n",
        "def translate(sentence):\n",
        "    seq = tokenizer_eng.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_eng, padding='post')\n",
        "\n",
        "    # estado inicial do encoder\n",
        "    states = encoder_model.predict(seq, verbose=0)\n",
        "\n",
        "    # primeiro token do decoder\n",
        "    tgt_seq = np.array([[tokenizer_pt.word_index[\"start\"]]])\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for _ in range(max_pt):\n",
        "        output, h, c = decoder_model.predict([tgt_seq] + states, verbose=0)\n",
        "\n",
        "        token_id = np.argmax(output[0, -1, :])\n",
        "\n",
        "        if token_id == 0:\n",
        "            break\n",
        "\n",
        "        word = index_to_word_pt.get(token_id, \"\")\n",
        "\n",
        "        if word == \"end\":\n",
        "            break\n",
        "\n",
        "        result.append(word)\n",
        "\n",
        "        tgt_seq = np.array([[token_id]])\n",
        "        states = [h, c]\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6) Teste\n",
        "# -------------------------------------------------------------\n",
        "test_sentences = [\n",
        "    \"hi\",\n",
        "    \"i love you\",\n",
        "    \"good night\",\n",
        "    \"how are you\",\n",
        "]\n",
        "\n",
        "print(\"\\n===== TESTES DE TRADUÇÃO =====\")\n",
        "for s in test_sentences:\n",
        "    print(f\"EN: {s}\")\n",
        "    print(f\"PT: {translate(s)}\\n\")\n"
      ]
    }
  ]
}